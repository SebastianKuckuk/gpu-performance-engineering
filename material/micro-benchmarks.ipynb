{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2c4a49-4064-41c2-a50a-3f5983f6bd7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Micro Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368b190-2560-4d74-a825-2281bae2397f",
   "metadata": {},
   "source": [
    "Micro benchmarks are essential for:\n",
    "* Gaining insights into hardware behavior and performance characteristics.\n",
    "* Identifying realistic performance boundaries.\n",
    "* Serving as simplified proxies for analyzing complex applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b5fc9",
   "metadata": {},
   "source": [
    "All benchmark implementations discussed in this lab are available as a CPU serial base version as well as GPU-accelerated versions based on CUDA, OpenMP and OpenACC.\n",
    "Implementations based on additional GPU programming approaches can be found online in the [Accelerated Programming EXamples (APEX)](https://github.com/SebastianKuckuk/apex) and [APEX Generator](https://github.com/SebastianKuckuk/apex-generator) repositories.\n",
    "The latter also contains automatic benchmarking capabilities which where used to obtain the data presented in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b2b315",
   "metadata": {},
   "source": [
    "All discussed performance results have been obtained using the CUDA version.\n",
    "Their OpenMP and OpenACC counterparts *generally* show the same performance characteristics, but smaller differences may arise.\n",
    "Using the APEX generator and included scripts, re-running the benchmarks should be easily possible for various GPU programming approaches and GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbbb25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Stream Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a116b269",
   "metadata": {},
   "source": [
    "We begin with a simple vector copy benchmark to establish realistic performance limits caused by data transfers to and from main memory (as well as the L2 cache at lower problem sizes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcdd2c6",
   "metadata": {},
   "source": [
    "\n",
    "The course material includes a CPU serial base version as well as GPU-accelerated versions based on CUDA, OpenMP and OpenACC:\n",
    "* [stream-base.cpp](../src/stream/stream-base.cpp),\n",
    "* [stream-cuda-expl.cu](../src/stream/stream-cuda-expl.cu),\n",
    "* [stream-omp-target-expl.cpp](../src/stream/stream-omp-target-expl.cpp), and\n",
    "* [stream-openacc-expl.cpp](../src/stream/stream-openacc-expl.cpp).\n",
    "\n",
    "Compilation and execution can be done with the below cells.\n",
    "\n",
    "As before, parameterization via command line arguments is possible:\n",
    "- **Data type**: `float` or `double`\n",
    "- **nx**: number of elements to be copied\n",
    "- **nWarmUp**: Number of non-timed warm-up iterations\n",
    "- **nIt**: Number of timed iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ada022",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 ../src/stream/stream-base.cpp -o ../build/stream-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stream-base double $((64 * 1024 * 1024)) 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afb73b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2019002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -fast -std=c++17 -o ../build/stream-cuda-expl ../src/stream/stream-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stream-cuda-expl double $((1024 * 1024 * 1024)) 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561a0836",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OpenMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0fb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -mp=gpu -target=gpu -o ../build/stream-omp-target-expl ../src/stream/stream-omp-target-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c3797",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stream-omp-target-expl double $((1024 * 1024 * 1024)) 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda07e16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OpenACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -acc=gpu -target=gpu -o ../build/stream-openacc-expl ../src/stream/stream-openacc-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stream-openacc-expl double $((1024 * 1024 * 1024)) 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fb50a",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a78a2",
   "metadata": {},
   "source": [
    "<img src=\"img/stream-A40.png\" alt=\"A40 stream results\" width=\"512px\" style=\"background-color:white\"/>\n",
    "<img src=\"img/stream-A100-SXM4-80GB.png\" alt=\"A100 stream results\" width=\"512px\" style=\"background-color:white\"/>\n",
    "<img src=\"img/stream-H100.png\" alt=\"H100 stream results\" width=\"512px\" style=\"background-color:white\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2402a016",
   "metadata": {},
   "source": [
    "The following table compares the theoretical bandwidth limits reported by the vendor (also compare the [GPU Architecture](./gpu-architecture.ipynb) notebook) with the asymptotic bandwidth observed (using the higher value in case of multiple ones) and the maximum observed bandwidth.\n",
    "\n",
    "| GPU         | theoretical BW | asymptotic BW |   max BW   |\n",
    "| ----------- | :------------: | :-----------: | :--------: |\n",
    "| A40         |    696 GB/s    |   ~650 GB/s   | ~1500 GB/s |\n",
    "| A100 (80GB) |   2039 GB/s    |  ~1750 GB/s   | ~2600 GB/s |\n",
    "| H100        |   2400 GB/s    |  ~2200 GB/s   | ~4100 GB/s |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e6832",
   "metadata": {},
   "source": [
    "Based on the results, we can see different reoccurring patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1bb888",
   "metadata": {},
   "source": [
    "**Slow ramp up behavior**\n",
    "\n",
    "As discussed previously, GPUs rely on massive parallelism.\n",
    "One reason for this is the oversubscription of resources to achieve *latency hiding*.\n",
    "Without a sufficient number of threads (bytes in flight), overall sustained bandwidth decreases.\n",
    "Nevertheless, this benchmark can already reveal expected performance for strongly under-utilizing GPU workloads.\n",
    "And it can be used to set up more tailored performance models, including roofline variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cc252",
   "metadata": {},
   "source": [
    "**Theoretical bandwidth limits are not reached**\n",
    "\n",
    "The actual achievable bandwidth can heavily rely on the number of bytes read and written for each thread/ element.\n",
    "Doing a copy benchmark is only one option (with 4 or 8 bytes read and written respectively).\n",
    "Other common options are:\n",
    "\n",
    "| Pattern | Formula                                         | Elements per LUP |\n",
    "| ------- | ----------------------------------------------- | ---------------- |\n",
    "| init    | A[i] = c                                        | 1 store          |\n",
    "| read    | discard = A[i]                                  | 1 load           |\n",
    "| scale   | A[i] = B[i] * c                                 | 1 load, 1 store  |\n",
    "| triad   | A[i] = B[i] + D[i] * C[i]                       | 3 load, 1 store  |\n",
    "\n",
    "as well as stencil-like patterns which we will pick up again later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef692fd",
   "metadata": {},
   "source": [
    "**Theoretical limits are exceeded**\n",
    "\n",
    "Bandwidth tests frequently feature a distinct bump where the approximated bandwidth exceeds theoretical limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360f42b",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4af7d",
   "metadata": {},
   "source": [
    "Check the raw performance data and isolate the locations of the bumps.\n",
    "Can you relate these numbers to the hardware characteristics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7657c7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1892c6",
   "metadata": {},
   "source": [
    "Checking where the bumps are located, and relating the corresponding total data sizes to the L2 cache sizes reveals the reason for the observed bandwidth bumps.\n",
    "\n",
    "| GPU         | theoretical BW |   max BW   |       occurred at       | L2 Cache |\n",
    "| ----------- | :------------: | :--------: | :---------------------: | :------: |\n",
    "| A40         |    696 GB/s    | ~1500 GB/s | ~350k elements (5.6 MB) |   6 MB   |\n",
    "| A100 (80GB) |   2039 GB/s    | ~2600 GB/s | ~1.3m elements (21 MB)  |  40 MB   |\n",
    "| H100        |   2400 GB/s    | ~4100 GB/s | ~2.0m elements (32 MB)  |  50 MB   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3b932",
   "metadata": {},
   "source": [
    "Note: while this may suggest, that we implemented an L2 bandwidth benchmark, the obtained results have to be regarded with caution.\n",
    "One reason is that they are usually very sensitive to varying *execution configurations*.\n",
    "A more carefully designed benchmark that repeatedly and redundantly reads data that fits into the target cache size is usually a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f207a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## FMA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabffd2b",
   "metadata": {},
   "source": [
    "Next, we complement the stream benchmark and its results by analyzing floating-point performance.\n",
    "Most modern hardware provides built-in support for *fused multiply-add (FMA)* operations.\n",
    "Check out on of the below benchmark variants - each repeats a fixed number of FMAs per cell to generate a sufficient numerical workload to saturate the hardware capabilities.\n",
    "* [fma-base.cpp](../src/fma/fma-base.cpp),\n",
    "* [fma-cuda-expl.cu](../src/fma/fma-cuda-expl.cu),\n",
    "* [fma-omp-target-expl.cpp](../src/fma/fma-omp-target-expl.cpp), and\n",
    "* [fma-openacc-expl.cpp](../src/fma/fma-openacc-expl.cpp).\n",
    "\n",
    "Compilation and execution can be done with the below cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e1ef7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d045a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 ../src/fma/fma-base.cpp -o ../build/fma-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a6ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/fma-base float $((1024)) 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c195fbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b891a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -fast -std=c++17 -o ../build/fma-cuda-expl ../src/fma/fma-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/fma-cuda-expl float $((1024 * 1024)) 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb610f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OpenMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8024599",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -mp=gpu -target=gpu -o ../build/fma-omp-target-expl ../src/fma/fma-omp-target-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/fma-omp-target-expl float $((1024 * 1024)) 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de68640",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OpenACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5524c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -acc=gpu -target=gpu -o ../build/fma-openacc-expl ../src/fma/fma-openacc-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/fma-openacc-expl float $((1024 * 1024)) 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3be33",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2ee19",
   "metadata": {},
   "source": [
    "<img src=\"img/fma-A40.png\" alt=\"A40 fma results\" width=\"512px\" style=\"background-color:white\"/>\n",
    "<img src=\"img/fma-A100-SXM4-80GB.png\" alt=\"A100 fma results\" width=\"512px\" style=\"background-color:white\"/>\n",
    "<img src=\"img/fma-H100.png\" alt=\"H100 fma results\" width=\"512px\" style=\"background-color:white\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea5f69",
   "metadata": {},
   "source": [
    "Comparing the measured performance to the theoretical maxima yields\n",
    "\n",
    "| GPU         | asymptotic float | theor. limit | asymptotic double | theor. limit |\n",
    "| ----------- | :--------------: | :----------: | :---------------: | :----------: |\n",
    "| A40         |       ~19        |      37      |       ~0.58       |     0.59     |\n",
    "| A100 (80GB) |       ~19        |      19      |       ~9.7        |     9.75     |\n",
    "| H100        |       ~34        |      67      |        ~22        |      34      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbde6f",
   "metadata": {},
   "source": [
    "**Single vs. double precision performance**\n",
    "\n",
    "The performance for different data types closely follows the hardware capabilities discussed in the [GPU Architecture](./gpu-architecture.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a66dd6",
   "metadata": {},
   "source": [
    "**Sawtooth behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e67bb",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dde4da",
   "metadata": {},
   "source": [
    "Take another look at the raw performance data.\n",
    "Can you identify the problem sizes related to the sawtooth spikes?\n",
    "How can they be connected to hardware properties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9010de1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c80a30",
   "metadata": {},
   "source": [
    "Performance is best when the number of threads is just below a multiple of the *wave size*, i.e. the number threads required to fill the current GPU entirely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b80a4",
   "metadata": {},
   "source": [
    "The observed performance pattern can be attributed to load imbalances usually referred to as *partial waves*.\n",
    "Additional threads go to a new wave which, if not filled, counts as partial.\n",
    "In case of equal execution time per thread, as here, the overall run time scales with the number of (partial) waves.\n",
    "Since the amount of FLOPs performed does not, at least not directly, the overall *throughput* decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d94f93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Strided Stream Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a60886",
   "metadata": {},
   "source": [
    "Next, we extend our previous stream benchmark to investigate the effect of different memory access patterns.\n",
    "For this, we add optional **strides** when **reading** from the input vector and/ or when **writing** to the output vector.\n",
    "The overall amount of elements read and written is kept *independent of the stride*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be66ef6",
   "metadata": {},
   "source": [
    "As with the base stream benchmark, this course includes different versions\n",
    "* [stream-strided-base.cpp](../src/stream-strided/stream-strided-base.cpp),\n",
    "* [stream-strided-cuda-expl.cu](../src/stream-strided/stream-strided-cuda-expl.cu),\n",
    "* [stream-strided-omp-target-expl.cpp](../src/stream-strided/stream-strided-omp-target-expl.cpp), and\n",
    "* [stream-strided-openacc-expl.cpp](../src/stream-strided/stream-strided-openacc-expl.cpp).\n",
    "\n",
    "The extended list of command line arguments is now:\n",
    "- **Data type**: `float` or `double`\n",
    "- **nx**: number of elements to be copied\n",
    "- **strideRead**: stride applied when reading data\n",
    "- **strideWrite**: stride applied when writing data\n",
    "- **nWarmUp**: Number of non-timed warm-up iterations\n",
    "- **nIt**: Number of timed iterations\n",
    "\n",
    "Compilation and execution can be done with the below cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ad69a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccabcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 ../src/stream-strided/stream-strided-base.cpp -o ../build/stream-strided-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stream-strided-base double $((64 * 1024 * 1024)) 2 1 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dc1e66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -fast -std=c++17 -o ../build/stream-strided-cuda-expl ../src/stream-strided/stream-strided-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stream-strided-cuda-expl double $((1024 * 1024 * 1024)) 2 1 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb95b238",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OpenMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -mp=gpu -target=gpu -o ../build/stream-strided-omp-target-expl ../src/stream-strided/stream-strided-omp-target-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fad5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stream-strided-omp-target-expl double $((1024 * 1024 * 1024)) 2 1 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dc4272",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OpenACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -acc=gpu -target=gpu -o ../build/stream-strided-openacc-expl ../src/stream-strided/stream-strided-openacc-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/stream-strided-openacc-expl double $((1024 * 1024 * 1024)) 2 1 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe00eb9",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018641b6",
   "metadata": {},
   "source": [
    "<img src=\"img/stream-strided-A40.png\" alt=\"A40 strided stream results\" width=\"512px\" style=\"background-color:white\"/>\n",
    "<img src=\"img/stream-strided-A100-SXM4-80GB.png\" alt=\"A100 strided stream results\" width=\"512px\" style=\"background-color:white\"/>\n",
    "<img src=\"img/stream-strided-H100.png\" alt=\"H100 strided stream results\" width=\"512px\" style=\"background-color:white\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa423cb",
   "metadata": {},
   "source": [
    "**Decreased Throughput**\n",
    "\n",
    "Generally, we distinguish two access patterns:\n",
    "* *Coalesced* accesses: consecutive threads access consecutive memory locations.\n",
    "* *Uncoalesced* accesses: consecutive threads access non-consecutive memory locations, either with strides or fully random.\n",
    "\n",
    "The main issue stems from the way GPUs transfer memory between the different stages of the memory hierarchy.\n",
    "In particular, transferring single bytes is generally not possible.\n",
    "Instead, the GPU works with chunks of differing granularity with **up to 128 bytes**.\n",
    "They can be referred to differently (e.g. cache line, sector, transfer, transaction) and may depend on different factors (e.g. the GPU in use, the source and destination as well as the operation).\n",
    "The following figures illustrate the issue - with 'less' coalesced accesses, more unused data has to be transferred which reduces the effective bandwidth.\n",
    "\n",
    "**A fully coalesced access** results in optimal performance.\n",
    "\n",
    "<img src=\"img/coalesced-access.png\" alt=\"coalesced access\" width=\"512px\" style=\"background-color:white\"/>\n",
    "\n",
    "**A partly coalesced access** with a stride of two results in doubling the number of bytes transferred and observed bandwidth halved.\n",
    "\n",
    "<img src=\"img/stride-2-access.png\" alt=\"stride 2 access\" width=\"512px\" style=\"background-color:white\"/>\n",
    "\n",
    "**A fully uncoalesced access** results in a massive drop in performance.\n",
    "\n",
    "<img src=\"img/uncoalesced-access.png\" alt=\"uncoalesced access\" width=\"512px\" style=\"background-color:white\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e001d",
   "metadata": {},
   "source": [
    "**Higher Impact on Write Operations**\n",
    "\n",
    "When all bytes of chunks are written, then the chunk can be put directly into L2.\n",
    "If, however, not all bytes are written, e.g. due to uncoalesced accesses, then the chunk has to be filled with the original values from DRAM first, before being partially overwritten in cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3de76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Strided FMA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8133ed7",
   "metadata": {},
   "source": [
    "Lastly, we extend our previously introduced FMA Benchmark to also include a potential stride.\n",
    "While this shares certain similarities with the strided stream benchmark, the effect investigated is a different one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec8f5d",
   "metadata": {},
   "source": [
    "As with the original fma benchmark, this course includes different versions\n",
    "* [fma-strided-base.cpp](../src/fma-strided/fma-strided-base.cpp),\n",
    "* [fma-strided-cuda-expl.cu](../src/fma-strided/fma-strided-cuda-expl.cu),\n",
    "* [fma-strided-omp-target-expl.cpp](../src/fma-strided/fma-strided-omp-target-expl.cpp), and\n",
    "* [fma-strided-openacc-expl.cpp](../src/fma-strided/fma-strided-openacc-expl.cpp).\n",
    "\n",
    "Compilation and execution can be done with the below cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7956148",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -O3 -march=native -std=c++17 ../src/fma-strided/fma-strided-base.cpp -o ../build/fma-strided-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/fma-strided-base float $((1024)) 2 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac7522",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a379871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -fast -std=c++17 -o ../build/fma-strided-cuda-expl ../src/fma-strided/fma-strided-cuda-expl.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e13830",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/fma-strided-cuda-expl float $((1024 * 1024)) 2 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7255d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OpenMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -mp=gpu -target=gpu -o ../build/fma-strided-omp-target-expl ../src/fma-strided/fma-strided-omp-target-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/fma-strided-omp-target-expl float $((1024 * 1024)) 2 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30c344",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### OpenACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658abe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvc++ -O3 -std=c++17 -acc=gpu -target=gpu -o ../build/fma-strided-openacc-expl ../src/fma-strided/fma-strided-openacc-expl.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "!../build/fma-strided-openacc-expl float $((1024 * 1024)) 2 2 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59be58a",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360098b5",
   "metadata": {},
   "source": [
    "<img src=\"img/fma-strided-A40.png\" alt=\"A40 strided fma results\" width=\"512px\" style=\"background-color:white\"/>\n",
    "<img src=\"img/fma-strided-A100-SXM4-80GB.png\" alt=\"A100 strided fma results\" width=\"512px\" style=\"background-color:white\"/>\n",
    "<img src=\"img/fma-strided-H100.png\" alt=\"H100 strided fma results\" width=\"512px\" style=\"background-color:white\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de52ae",
   "metadata": {},
   "source": [
    "**Idle threads waste performance**\n",
    "\n",
    "If only a subset of threads in a warp participate in arithmetic operations, some of the execution units can not perform meaningful work (assuming that non-participating threads cannot overlap other work).\n",
    "This directly relates to a scaled down performance.\n",
    "\n",
    "Note: branching itself is usually not harmful for performance, only the resulting *divergence* is.\n",
    "Moreover, modern GPUs support thread divergence which can help mitigate any negative implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea39b98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db64168d",
   "metadata": {},
   "source": [
    "After now having a better understanding of the GPU hardware and some common performance mitigating patterns, we resume our analysis of our 2D stencil application in the [Application Level Profiling](./application-level-profiling.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
