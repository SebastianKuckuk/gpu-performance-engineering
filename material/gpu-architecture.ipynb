{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba76d3ac-bbd1-45dc-bd3e-6fa68d8215e4",
   "metadata": {},
   "source": [
    "# GPU Architecture Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b519c96-e975-4b1e-8439-82cdcc3bdf48",
   "metadata": {},
   "source": [
    "Before diving into performance analysis, we must understand how GPUs work. Key performance factors include:\n",
    "* **Memory bandwidth:** How quickly data can be moved between memory and compute units.\n",
    "* **Compute throughput:** How many floating-point or integer operations can be performed per second.\n",
    "* **Occupancy:** Ratio of active threads to maximum threads supported by the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c82cdbc-b690-4f66-bc39-e62643431fd2",
   "metadata": {},
   "source": [
    "## Example: NVIDIA H100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932baf6-78c9-4400-8a70-2aae441f5f48",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/h100-chip.png\" alt=\"H100 Chip\" width=\"384px\"/>\n",
    "\n",
    "A key design principle of all GPUs is their **hierarchical structure**, which is essential for achieving high levels of parallelism and scalability.\n",
    "To understand GPU performance, we begin by examining the **NVIDIA H100 architecture**, a state-of-the-art example of modern GPUs.\n",
    "The H100 demonstrates the hierarchical design and advanced capabilities that drive GPU performance.\n",
    "\n",
    "For further details, refer to the official [NVIDIA H100 White Paper](https://resources.nvidia.com/en-us-tensor-core).\n",
    "The following figures and information are derived from this resource as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef089229-f1cf-4761-a195-4650efe0411e",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/h100-layout-annotated.png\" alt=\"H100 Chip Layout with Annotations\" width=\"768px\" style=\"background-color:white\"/>\n",
    "\n",
    "The diagram on the right illustrates a 'full configuration' of one H100 chip, emphasizing the hierarchical arrangement of its components:\n",
    "* **Graphics Processing Clusters (GPCs)** with multiple\n",
    "* **Texture Processing Clusters (TPCs)** with multiple\n",
    "* **Streaming Multiprocessors (SMs)** with multiple\n",
    "* **SM Sub-Partitions (SMSPs)** (see below).\n",
    "\n",
    "In practice, not all units in the full configuration are available. For example, the SXM5 version of the H100 includes:\n",
    "* 8 GPCs, each with\n",
    "* 8 or 9 TPCs, each with\n",
    "* 2 SMs, each with\n",
    "* 4 SMSPs,\n",
    "for a total of **132 Streaming Multiprocessors (SMs)** available for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4102873-f81c-4215-859f-db507a5f02ee",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/h100-sm-layout.png\" alt=\"H100 SM Layout\" width=\"384px\" style=\"background-color:white\"/>\n",
    "\n",
    "Each SM is further subdivided into 4 sub partitions, as visualized in the figure on the right, each with\n",
    "* 16 INT32 units, for a total of 132 * 4 * 16 = 8448,\n",
    "* 32 FP32 units, for a total of 132 * 4 * 32 = 16896,\n",
    "* 16 FP64 units, for a total of 132 * 4 * 16 = 8448, and\n",
    "* 1 tensor core, for a total of 132 * 4 = 528.\n",
    "Each unit is capable of executing one fused-multiply-add (FMA) operation per cycle, with the exception of the tensor cores which can each perform 512 FP16/FP32-mixed-precision FMAs at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4687a4e-ea46-4a3c-9694-74bf1a548eb1",
   "metadata": {},
   "source": [
    "## Theoretical Peak Performance of a GPU\n",
    "\n",
    "To evaluate the computational capabilities of a GPU, we calculate its **theoretical peak performance**. The formula is:\n",
    "\n",
    "$$p_{\\text{peak}} = n_{\\text{cores}} \\cdot \\frac{n_{\\text{inst}}}{cy} \\cdot \\frac{n_{\\text{OP}}}{\\text{inst}} \\cdot clk$$\n",
    "\n",
    "Where:\n",
    "* **$n_{\\text{cores}}$**: The total number of execution units (cores) in the GPU.\n",
    "* **$\\frac{n_{\\text{inst}}}{cy}$**: The number of instructions issued per cycle.\n",
    "* **$\\frac{n_{\\text{OP}}}{\\text{inst}}$**: The number of operations, usually floating-point operations (FLOPs), executed per instruction, determined by:\n",
    "  * An **FMA factor** (usually 2), as one fused multiply-add (FMA) instruction performs two operations.\n",
    "  * The **SIMD width**, representing the number of parallel operations per instruction.\n",
    "* **$clk$:** The clock rate or frequency of the GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfeeea1-dd77-4e1c-ae1b-4c6c5e66e57b",
   "metadata": {},
   "source": [
    "For GPUs, performance modeling can adopt different perspectives depending on how the architecture is abstracted.\n",
    "Below, we calculate $p_{\\text{peak}}$ for **single-precision floating-point operations** for one H100 GPU using three views:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3056eb-784f-48a2-b122-15cff87c0c28",
   "metadata": {},
   "source": [
    "#### 1. Execution Units as Cores\n",
    "\n",
    "Each execution unit is modeled as a single core:\n",
    "- Total execution units: $132 \\, \\text{SMs} \\times 4 \\, \\text{Sub-Partitions per SM} \\times 32 \\, \\text{FP32 units per Sub-Partition} = 16\\,896 \\, \\text{execution units}$.\n",
    "- Instructions per cycle: $1$.\n",
    "- FMA factor: $2$ (each FMA performs two FLOPs).\n",
    "- Clock speed: $1.84 \\, \\text{GHz}$.\n",
    "\n",
    "$$p_{\\text{peak}} = 16\\,896 \\cdot 1 \\cdot 2 \\cdot 1.84 \\, \\text{GF/s} = 62 \\, \\text{TF/s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b1847-6ce9-4a6b-9408-e0cee7e3c69a",
   "metadata": {},
   "source": [
    "#### 2. SMSPs as Cores\n",
    "\n",
    "Each **Streaming Multiprocessor Sub-Partition (SMSP)** is modeled as a core with a SIMD width of 32:\n",
    "- Total SMSPs: $132 \\, \\text{SMs} \\times 4 \\, \\text{SMSPs per SM} = 528 \\, \\text{SMSPs}$.\n",
    "- SIMD width: $32$.\n",
    "- Instructions per cycle: $1$.\n",
    "- FMA factor: $2$.\n",
    "- Clock speed: $1.84 \\, \\text{GHz}$.\n",
    "\n",
    "$$p_{\\text{peak}} = 528 \\cdot 1 \\cdot (2 \\cdot 32) \\cdot 1.84 \\, \\text{GF/s} = 62 \\, \\text{TF/s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f085d-9595-47ae-a7df-3b7b622467e6",
   "metadata": {},
   "source": [
    "#### 3. SMs as Cores\n",
    "\n",
    "Each **Streaming Multiprocessor (SM)** is modeled as a core capable of issuing 4 instructions per cycle, with a SIMD width of 32:\n",
    "- Total SMs: $132$.\n",
    "- Instructions per cycle: $4$.\n",
    "- SIMD width: $32$.\n",
    "- FMA factor: $2$.\n",
    "- Clock speed: $1.84 \\, \\text{GHz}$.\n",
    "\n",
    "$$p_{\\text{peak}} = 132 \\cdot 4 \\cdot (2 \\cdot 32) \\cdot 1.84 \\, \\text{GF/s} = 62 \\, \\text{TF/s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59151232-69c3-4668-8050-cfe9b488ff04",
   "metadata": {},
   "source": [
    "#### Note on SIMD Width and Warp Size\n",
    "\n",
    "We determine the **SIMD width** according to the number of FP32 execution units in the GPU.\n",
    "Alternatively, the **warp size** (typically 32 for NVIDIA GPUs) can be used as a basis for modeling.\n",
    "When using the warp size, the **instructions per cycle** must be adjusted and it may be less than $1$ if multiple cycles are required to finish the computations for an entire warp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088f26e-4eea-46c2-b658-3dd95e7e4c3f",
   "metadata": {},
   "source": [
    "## Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e49fe90-d838-4779-a983-498e253a4f2d",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/h100-memory-abstract.png\" alt=\"memory abstraction\" width=\"512px\"/>\n",
    "\n",
    "The GPU memory system is organized into a hierarchy designed to balance capacity, speed, and access latency.\n",
    "Its levels, at the example of the H100, are (latency and bandwidth values provided by [GPU Benches](https://github.com/RRZE-HPC/gpu-benches))\n",
    "- **Registers**\n",
    "    - Scope: Private to each thread.\n",
    "    - Latency: Typically 1 cycle.\n",
    "    - Capacity: Each SM has 256 KB available for registers.\n",
    "- **L1 Cache** and **Shared Memory**\n",
    "    - Scope: Shared among all threads scheduled on a given SM. Threads of a given block can access the same shared memory.\n",
    "    - Latency: ~32 cycles.\n",
    "    - Capacity: Configurable up to 100 KB per SM for shared memory, with the remaining space used for L1 cache (up to 256 KB per SM).\n",
    "- **L2 Cache**\n",
    "    - Scope: Shared among all threads (GPU-wide)\n",
    "    - Latency: ~280 cycles.\n",
    "    - Capacity: 50 MB (two partitions with 25 MB each)\n",
    "- **Global Memory (DRAM)**\n",
    "    - Scope: Global to all threads.\n",
    "    - Latency: ~690 cycles.\n",
    "    - Capacity: 80 or 96 GB\n",
    "    - Bandwidth: up to 3.35 TB/s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c4b036-cfc7-4589-9087-6e734074aef6",
   "metadata": {},
   "source": [
    "<img align=\"right\" src=\"img/interconnect.png\" alt=\"interconnect\" width=\"384px\"/>\n",
    "\n",
    "Beyond accessing data already residing in GPU memory, data transfers between host and device as well as between devices are necessary.\n",
    "These connection paths are usually much slower and can be serious bottlenecks in applications.\n",
    "\n",
    "| Connection Type  | Bandwidth             | Direction     | Purpose                                   |\n",
    "|------------------|-----------------------|---------------|-------------------------------------------|\n",
    "| **DRAM**         | 3.36 TB/s             | Bidirectional | Internal memory access for GPU operations |\n",
    "| **PCIe 5.0 x16** | 63 GB/s               | Per direction | Communication with CPU                    |\n",
    "| **NVLink**       | 450 GB/s              | Per direction | High-speed data sharing between GPUs      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b00746",
   "metadata": {},
   "source": [
    "## Comparison of Different GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89446378",
   "metadata": {},
   "source": [
    "### NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb6727",
   "metadata": {},
   "source": [
    "|                             |  V100 PCIe 32 GB   |      RTX 3080       |       A40 PCIe       | A100 SXM 40 GB \\| 80 GB |    H100 SXM 94 GB     |    H100 PCIe 96 GB    |\n",
    "| --------------------------- | :----------------: | :-----------------: | :------------------: | :---------------------: | :-------------------: | :-------------------: |\n",
    "| Availability                |  NHR@FAU TinyGPU   |   NHR@FAU TinyGPU   |     NHR@FAU Alex     |      NHR@FAU Alex       |     NHR@FAU Helma     |          ---          |\n",
    "| CUDA                        |        7.0         |         8.6         |         8.6          |           8.0           |          9.0          |          9.0          |\n",
    "| #Cores                      | 5120<br/>(80 * 64) | 8704<br/>(68 * 128) | 10752<br/>(84 * 128) |   6912<br/>(108 * 64)   | 16896<br/>(132 * 128) | 16896<br/>(132 * 128) |\n",
    "| FP32 Performance \\[TFLOPS\\] |         14         |         30          |          37          |           19            |          67           |          62           |\n",
    "| FP64 Performance \\[TFLOPS\\] |         7          |        0.47         |         0.59         |          9.75           |          34           |          31           |\n",
    "| FP64:FP32 Ratio             |        1:2         |        1:64         |         1:64         |           1:2           |          1:2          |          1:2          |\n",
    "| Memory \\[GB\\]               |         32         |         10          |          48          |        40 \\| 80         |          94           |          96           |\n",
    "| Bandwidth \\[GB/s\\]          |        897         |         760         |         696          |      1555 \\| 2039       |         2400          |         3360          |\n",
    "| L2 Cache \\[MB\\]             |         6          |          5          |          6           |           40            |          50           |          50           |\n",
    "| TDP \\[W\\]                   |        250         |         320         |         300          |           400           |          700          |          700          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edd43d",
   "metadata": {},
   "source": [
    "### AMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb4754",
   "metadata": {},
   "source": [
    "|                             |        MI100         |        MI210         |        MI250X        |        MI300X        |        MI300A        |\n",
    "| --------------------------- | :------------------: | :------------------: | :------------------: | :------------------: | :------------------: |\n",
    "| Availability                | NHR@FAU Test Cluster | NHR@FAU Test Cluster |        LUMI-G        | NHR@FAU Test Cluster | NHR@FAU Test Cluster |\n",
    "| #Cores                      | 7680<br/>(120 * 64)  | 6656<br/>(104 * 64)  | 14080<br/>(220 * 64) | 19456<br/>(304 * 64) | 14592<br/>(228 * 64) |\n",
    "| FP32 Performance \\[TFLOPS\\] |          23          |          23          |          48          |         163          |         123          |\n",
    "| FP64 Performance \\[TFLOPS\\] |          12          |          23          |          48          |          82          |          61          |\n",
    "| FP64:FP32 Ratio             |         1:2          |         1:1          |         1:1          |         1:2          |         1:2          |\n",
    "| Memory \\[GB\\]               |          32          |          64          |         128          |         192          |         128          |\n",
    "| Bandwidth \\[GB/s\\]          |         1229         |         1638         |         3277         |         5300         |         5300         |\n",
    "| L2 Cache \\[MB\\]             |          8           |          16          |          16          |          16          |          4           |\n",
    "| Infinity Cache \\[MB\\]       |         ---          |         ---          |         ---          |         256          |         256          |\n",
    "| TDP \\[W\\]                   |         300          |         300          |         500          |         750          |         550          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4f928b",
   "metadata": {},
   "source": [
    "## H2D and D2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442a3d7",
   "metadata": {},
   "source": [
    "### NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d71134",
   "metadata": {},
   "source": [
    "|                                    |  Volta  | Ampere  | Hopper  | \n",
    "|------------------------------------|:-------:|:-------:|:-------:|\n",
    "| CUDA                               |   7.x   |   8.x   |   9.x   |\n",
    "| PCIe                               | 3.0 x16 | 4.0 x16 | 5.0 x16 |\n",
    "| Bandwidth (per direction) \\[GB/s\\] |  15.8   |  31.5   |  63.0   |\n",
    "| NVLink (per direction) \\[GB/s\\]    |   150   |   300   |   450   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2f14c",
   "metadata": {},
   "source": [
    "### AMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105b1ff",
   "metadata": {},
   "source": [
    "|                                                      |  MI100  |  MI210  | MI250X  |\n",
    "| ---------------------------------------------------- | :-----: | :-----: | :-----: |\n",
    "| PCIe                                                 | 4.0 x16 | 4.0 x16 | 4.0 x16 |\n",
    "| Bandwidth (per direction) \\[GB/s\\]                   |  31.5   |  31.5   |  31.5   |\n",
    "| Infinity Fabric (inside GPU, per direction) \\[GB/s\\] |   ---   |   ---   |   400   |\n",
    "| Infinity Fabric D-D (per direction) \\[GB/s\\]         |   100   |   150   |   300   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e8330",
   "metadata": {},
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bf077",
   "metadata": {},
   "source": [
    "Next, we will look at performance models to relate the theoretical peak performance to application characteristics.\n",
    "Head over to the [Performance Models](./performance-models.ipynb) notebook to get started."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
